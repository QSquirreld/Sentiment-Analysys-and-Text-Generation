{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFo7QR8GQmmz"
   },
   "source": [
    "## Идея:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUGkkHA_QqKP"
   },
   "source": [
    "**Идея:** реализовать автоматическую генерацию ответа к отзыву. Таким образом совместив задачу сентимент анализа и генерации текста.\n",
    "\n",
    "Используются предобученные модели: **RoBERTa** для анализа и **Mistral** для генерации.\n",
    "\n",
    "[Датасет](https://huggingface.co/datasets/Yelp/yelp_review_full):\n",
    "* содержит 650к/50к отзывов с платформы Yelp, об услугах, сервисах, ресторанах и пр.\n",
    "* имеет 5 классов — 5 звёзд\n",
    "* мультиязычный"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0gss5WacWxm"
   },
   "source": [
    "## Сентимент анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dW8oOckSWfas"
   },
   "source": [
    "### Загрузка и предварительная обработка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13214,
     "status": "ok",
     "timestamp": 1752185598260,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "HnDOcYfHGd_C",
    "outputId": "4916a802-e872-4574-dd6c-60557823d2a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets huggingface_hub fsspec;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5758,
     "status": "ok",
     "timestamp": 1752185604021,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "2Zlst_g7Dzof",
    "outputId": "94ed88b3-7ecf-4b76-efb2-f74982e14eed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'yelp_review_full' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'yelp_review_full' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"yelp_review_full\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "si33fmiyco33"
   },
   "source": [
    "Немного нормализуем датасет, для этого создадим соответствующие функции:\n",
    "1. Очистка текста от лишних символов, эмодзи и прочего"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BV1ojNPvEENJ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,!?;:()\\\"'-]\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXZ3-JPmdiqY"
   },
   "source": [
    "2. Переразметим данные с оценки 0-5, до 0-3(отрицательный, нейтральный и положительный)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUDWW2IhbrHV"
   },
   "outputs": [],
   "source": [
    "def convert_label(example):\n",
    "    rating = example[\"label\"] + 1\n",
    "    if rating in [4, 5]:\n",
    "        label = 2  # positive\n",
    "    elif rating == 3:\n",
    "        label = 1  # neutral\n",
    "    else:\n",
    "        label = 0  # negative\n",
    "    example[\"label\"] = label\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cl132MXcdkHu"
   },
   "source": [
    "Тут появилась идея оставить только англоязычные тексты, т.к. латиница != английский. Но посмотрев сколько будет это всё обрабатываться, стало не так важно, хотя думаю это повысило бы метрики.\n",
    "\n",
    "Возможно стоило бы сократить датасет раз в 10 хотя бы, но когда я это делал, думал, о том, что вдруг все попавшие в выборку отзывы будут на любом кроме английского(азиатов же много..) 🤡🤡\n",
    "\n",
    "Тем не менее, некоторый функционал реализовал, осталось буквально выборку сделать и протестить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJLKTaOANXAI"
   },
   "outputs": [],
   "source": [
    "# !pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcXYFtjQNGR0"
   },
   "outputs": [],
   "source": [
    "# from langdetect import detect\n",
    "\n",
    "# def is_english(example):\n",
    "#     try:\n",
    "#         return detect(example['text']) == 'en'\n",
    "#     except:\n",
    "#         return False  # если не удалось определить язык, исключаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ic0K3bEINFSA"
   },
   "outputs": [],
   "source": [
    "# ds = ds.filter(is_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kt8_TglWeqQP"
   },
   "source": [
    "Здесь сократили тренировочный и и валидационный датасет, т.к. 700к отзывов собирались обучатся 5 дней на встроенном в коллабе железе.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ng-2-AL0c38"
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, concatenate_datasets\n",
    "\n",
    "#ds = DatasetDict({\n",
    "#    'train': ds['train'].shuffle(seed=42).select(range(13000)),\n",
    "#    'test': ds['test'].shuffle(seed=42).select(range(1000))\n",
    "#})\n",
    "\n",
    "train_ds = ds['train']\n",
    "\n",
    "pos = train_ds.filter(lambda x: x['label'] in [3, 4])\n",
    "neu = train_ds.filter(lambda x: x['label'] == 2)\n",
    "neg = train_ds.filter(lambda x: x['label'] in [0, 1])\n",
    "\n",
    "pos = pos.shuffle(seed=42).select(range(5000))\n",
    "neu = neu.shuffle(seed=42).select(range(5000))\n",
    "neg = neg.shuffle(seed=42).select(range(5000))\n",
    "test_ds = ds['test'].shuffle(seed=42).select(range(2500))\n",
    "\n",
    "balanced_train = concatenate_datasets([pos, neu, neg]).shuffle(seed=42)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': balanced_train,\n",
    "    'test': test_ds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "2c3e8c0062d940ef85f3208379f2619f",
      "be8891b869054725ac6093261d82fa74",
      "2bae2173d92e4f159d0b2d50c1904a73",
      "7401321685ad46e8b57cc6636dda2672",
      "a8a14ba6be164d038749fc973a8f9084",
      "f0c378808bae4cd0a4fc33093fe1d17a",
      "508d9d79b8cb495d9799d682e97ab151",
      "7449f57aa1e0480b9f10cc11ddcecf3e",
      "16b46b84061e47aca690135e71adce16",
      "8c3318b308024c0d9aa51f81936a7458",
      "39231a21a0a9441088c3efdbba8af8e8",
      "d642840e727d4462a9065ac8a3a37786",
      "faaf8812ec2f4ba281246a50e0b71185",
      "037dd14e3c654d0e818bc9ac23f3f24a",
      "f6f1c4f56fc740f3a14ab09baf476cb9",
      "8ba2ba323e6144d5884511f015436557",
      "5eac28539ad447cd937067a3d7f9747b",
      "8ad344b144ec4873a9287b54f35ce80e",
      "560b736e556c4c49a0b3c33b003acda7",
      "d6b65d97a1cb492fba886b9e0c02a11c",
      "4d62b50963c14a2e8b560f6301b27348",
      "4d485407eb5c4b9296cf0ad6d617a7e4",
      "b50422aa594b46089ef2204ecf7cc875",
      "ce87fa4a31954e9fa101d876ca7f9a36",
      "28092c8d7a714ec2a2e356c35ca90ecb",
      "beaeab966dd44275904dce3df8b2cc64",
      "f6f04c0eef6a4de0806cc373b96b0175",
      "3f35dfeae1dc4e4881f12231c43ef67f",
      "1bdb92d6d06141cabaaee2ae382767ff",
      "01fe552ef3a94a61a0f83d9de583b397",
      "e9694ef2655843d4857030eeb4f87f03",
      "5ecdcac29a3f43db860620b132501748",
      "9b7344bc717b4b7fa34f491556c5f0c6",
      "6b753ce30d324453974b5458883de305",
      "7c4d973e7eca466899234b28ba397bbc",
      "717e4f3be73e4572920858429f779ec2",
      "af5e581ceca04e499da5d12b5af15811",
      "7a0988408ee74ad78e53fd0d4c331036",
      "f9d10529374047dc96b1ab2551e8feed",
      "d367f4b310344781b8f24da92390c7e5",
      "18d8bf2117a04c4ea9c0f0df6723b17d",
      "223caf3b4e544fc5b2b3d229b0ec1c62",
      "bdfbdeb59f7e40118b4f71cb4982c141",
      "08a78870b6f74d68aceca6efeaf1d5b1"
     ]
    },
    "executionInfo": {
     "elapsed": 9991,
     "status": "ok",
     "timestamp": 1752185614455,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "6d5LRPXuLa8c",
    "outputId": "fbce79e7-a5c5-4c63-b908-e63b5247a95c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3e8c0062d940ef85f3208379f2619f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d642840e727d4462a9065ac8a3a37786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50422aa594b46089ef2204ecf7cc875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b753ce30d324453974b5458883de305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.map(lambda x: {\"text\": clean_text(x[\"text\"])})\n",
    "ds = ds.map(convert_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 893,
     "status": "ok",
     "timestamp": 1752185615350,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "ny4HzahP9-F1",
    "outputId": "e502c241-7b87-4e5f-d77c-93703d6bf4ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative: 5000 примеров\n",
      "neutral: 5000 примеров\n",
      "positive: 5000 примеров\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter(ds['train']['label'])\n",
    "label_names = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "for label_id, count in counter.items():\n",
    "    print(f\"{label_names[label_id]}: {count} примеров\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTZTjoL3Ws1x"
   },
   "source": [
    "### Подготовка токенизатора и DataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2GnP3SlgkVC"
   },
   "source": [
    "Изначально брали bert-base, но для повышения качества LLM предложила использовать roberta, я и послушался)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gY8lpMV6QRSg"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], max_length=128, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4QfHXtuinwl"
   },
   "source": [
    "Собственно токенизируем примеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "edab00ba0c47495a97b77ba0f020d2f2",
      "24929f7de270407a91603cf5ee0194ab",
      "5dd26e3b8c1a44e59dd23e92ec94d2ff",
      "f3a3b2ee38004521ae58e61e1a90f2ed",
      "dba3dd165a694b45b18858e02fee9f50",
      "b9ed70befad041a9adc11b20c6e8ac70",
      "f9492666fd134bb0ba07fa217070d835",
      "dbda5991288e4fbe922eb7596d90a678",
      "11b7436b33cd46639784eee59aab1dba",
      "17b10e86ac754c1fad5afb1aee31c64a",
      "7a09cec3773b4fdbb17bdbbec5651de6",
      "7fb081ce56df414fb76a76582b24b2ad",
      "9aa16d3d65384266a091445a1c4acf7c",
      "47f9dc063c024d7a8d52db1addef698e",
      "52b48502cbcc4843a7b3ca775acaad13",
      "68127836089e4507aae4089a3e97f821",
      "8cd807094a354c7bbefd05bff7f70617",
      "545119b614a746f6bd9df9d127e96b9d",
      "ab668d38647b4fb180f50900bd329346",
      "ec92220481f04f5fa8f61fe3069ea579",
      "3ae7cb2cca4844e68cf133329f1f4e4b",
      "bf44106616c74625bd985b86440bc94e"
     ]
    },
    "executionInfo": {
     "elapsed": 20058,
     "status": "ok",
     "timestamp": 1752185636345,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "OX6bY1C6j52Z",
    "outputId": "4986a216-e182-4d5c-a3ce-2eae4690a37e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edab00ba0c47495a97b77ba0f020d2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb081ce56df414fb76a76582b24b2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = ds.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kODznMS3kiJH"
   },
   "source": [
    "Добавим паддинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFxzOswNoVIl"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POv7mZL_W3gQ"
   },
   "source": [
    "### Настройка параметров обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 10067,
     "status": "ok",
     "timestamp": 1752185646453,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "FL8IQXSmoFNo",
    "outputId": "0efd70df-c9e5-4bc6-ced2-48a2142681f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GttFP_gblbtE"
   },
   "source": [
    "Указали метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVLlOiiQj8TS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcu_T6Dqlg4i"
   },
   "source": [
    "Для красивого вывода 🌠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rG0o2i_roDCp"
   },
   "outputs": [],
   "source": [
    "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1752185647168,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "n9EYs9AMrqid",
    "outputId": "90c204e4-a3c1-4c70-c6ff-29b41175d884"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZobaPjm-mApR"
   },
   "source": [
    "Model hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1752185647574,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "HD9WbkhHoyIW",
    "outputId": "4f19655b-ce66-4d13-eb8e-3c1388a7ae19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqvuUMOJr5Lm"
   },
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU2VRaOdmHBB"
   },
   "source": [
    "Параметры обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBWVUo8UpO94"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuGhWGxImTsc"
   },
   "source": [
    "Параметры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1752185650358,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "wYJgYk1CpkuV",
    "outputId": "4d123d9d-26c0-4846-c084-4091669793ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-226-3091136456.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svtQFjR0W_8T"
   },
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weNb6HxRmjSN"
   },
   "source": [
    "Тут регаться пришлось и апишки делать(("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "executionInfo": {
     "elapsed": 4106,
     "status": "error",
     "timestamp": 1752185654468,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "YQpSwMqisuq8",
    "outputId": "667f47b2-1c16-4537-fc25-71ff40335a13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/1175 : < :, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 143255 has 14.73 GiB memory in use. Of the allocated memory 14.36 GiB is allocated by PyTorch, and 245.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-227-4032920361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3744\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3745\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3747\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3808\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3810\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3811\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1203\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    870\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 )\n\u001b[1;32m    617\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    619\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 143255 has 14.73 GiB memory in use. Of the allocated memory 14.36 GiB is allocated by PyTorch, and 245.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFaH4sawXLPX"
   },
   "source": [
    "### Предсказание тональности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlogOo1zms5B"
   },
   "source": [
    "Создали пайп и загрузили лучшую модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjM7MC-LtyC7"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=trainer.model,           # используем обученную модель из Trainer\n",
    "    tokenizer=tokenizer,           # передаём токенизатор, связанный с моделью\n",
    "    device=0                      # явно указываем использовать GPU с индексом 0 (если доступен)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5O9SI9Vm30E"
   },
   "source": [
    "Используем валидационный датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2PbwZHJARec"
   },
   "outputs": [],
   "source": [
    "targets = ds[\"test\"][\"label\"]\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsPWuM05AWzS"
   },
   "outputs": [],
   "source": [
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "for out in pipe(KeyDataset(ds[\"test\"], \"text\"), batch_size=16, truncation=True):\n",
    "    predictions.append(label2id[out[\"label\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvkHZWqanu7s"
   },
   "source": [
    "Матрица ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "os-v-rSF1D3b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(targets, predictions))\n",
    "\n",
    "cm = confusion_matrix(targets, predictions)\n",
    "ConfusionMatrixDisplay(cm, display_labels=np.array(list(label2id.keys()))).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90YKLbtNn2c2"
   },
   "source": [
    "Тут получаем предсказание с пользовательским отзывом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSXWtstm1KEQ"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(text, pipeline):\n",
    "    \"\"\"\n",
    "    Получить предсказание сентимента для заданного текста.\n",
    "\n",
    "    Args:\n",
    "        text (str): Входной текст для анализа.\n",
    "        pipeline (transformers.Pipeline): Предобученный пайплайн для классификации текста.\n",
    "\n",
    "    Returns:\n",
    "        dict: Результат с ключами 'label' (название класса) и 'score' (уверенность).\n",
    "    \"\"\"\n",
    "    label_map = {\n",
    "    \"LABEL_0\": \"negative\",\n",
    "    \"LABEL_1\": \"neutral\",\n",
    "    \"LABEL_2\": \"positive\"\n",
    "    }\n",
    "\n",
    "    result = pipeline(text)[0]  # Получаем первый результат (один текст)\n",
    "    label = label_map.get(result['label'], result['label'])\n",
    "    score = result['score']\n",
    "    return {\"label\": label, \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwb4y3Hm2Hx4"
   },
   "outputs": [],
   "source": [
    "user_text = \"I love this product! It works great. You are best!\"\n",
    "\n",
    "prediction = predict_sentiment(user_text, pipe)\n",
    "print(f\"Sentiment: {prediction['label']}, Confidence: {prediction['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qmd6OYQGoBh5"
   },
   "source": [
    "На одной из попыток был 1.00.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-p-IJ5xXXqq"
   },
   "source": [
    "## Генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WG1mEu8UjmH"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KqYv9VISoC7"
   },
   "outputs": [],
   "source": [
    "#generator = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.1\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "qW-gVDJvXb1Y"
   },
   "outputs": [],
   "source": [
    "!pip install optimum optimum[onnxruntime] auto-gptq;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXDHYJO4rU2Q"
   },
   "source": [
    "Задаём **модель**\n",
    "\n",
    "Тут для использования Mistral, пришлось проводить аутентификации, запрашивать, создавать токен и тд.\n",
    "\n",
    "Как вариант ещё был gpt, но его недостаточно; Гугловский FLAN-T5, но тяжело, как и стандартный Mistral, поэтому взяли GPTQ, квантованную версию, чтобы она влезла в ресурсы колаба."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zv5H7MZeXRBH"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\", use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSMN5o9EsYOq"
   },
   "source": [
    "Создали, некоторые базовые **промпты**\n",
    "\n",
    "Примеры никак не повышают метрики :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEEZZQwYPedP"
   },
   "outputs": [],
   "source": [
    "def build_prompt(review_text, sentiment_label):\n",
    "    base_prompt = (\n",
    "        f'Review: \"{review_text}\"\\n'\n",
    "        f'You are a customer service assistant. '\n",
    "        f'Generate a short, polite and professional response to the review.\\n'\n",
    "    )\n",
    "\n",
    "    if sentiment_label == 'positive':\n",
    "        task_instruction = (\n",
    "            \"Thank the customer warmly for their feedback and encourage them to return or shop again.\\n\\n\"\n",
    "            # \"Example 1: 'Thank you so much for your kind words! We're thrilled you enjoyed your experience.'\\n\"\n",
    "            # \"Example 2: 'We really appreciate your feedback. Come back again soon!'\\n\"\n",
    "            # \"Example 3: 'Thanks for sharing your positive review! Hope to see you again.\\n'\"\n",
    "            # \"Example 5: 'So glad to hear you’re happy with the product. Thank you!'\\n\"\n",
    "            # \"Example 6: 'Thanks a lot! Your satisfaction means a lot to us.'\\n\\n\"\n",
    "            # \"Output only response, no extra text.\\n\\n\"\n",
    "            \"Response:[llm_response]\"\n",
    "        )\n",
    "    elif sentiment_label == 'neutral':\n",
    "        task_instruction = (\n",
    "            \"Thank the customer politely and let them know that their feedback will be considered to improve the service.\\n\\n\"\n",
    "            # \"Example 1: 'Thank you for your feedback. We'll take it into consideration.'\\n\"\n",
    "            # \"Example 2: 'We appreciate your input and will use it to improve our service.'\\n\"\n",
    "            # \"Example 3: 'Thanks for your comment. It helps us make things better.'\\n\"\n",
    "            # \"Example 4: 'We hear you and value your opinion. Thank you.'\\n\"\n",
    "            # \"Example 5: 'Your feedback has been noted. Thank you for taking the time.'\\n\\n\"\n",
    "            \"Output only response, no extra text.\\n\\n\"\n",
    "            \"Response:[llm_response]\"\n",
    "        )\n",
    "    else:  # negative\n",
    "        task_instruction = (\n",
    "            \"Apologize sincerely for the negative experience and ask how the service or product can be improved.\\n\\n\"\n",
    "            # \"Example 1: 'We're sorry to hear about your experience. How can we improve?'\\n\"\n",
    "            # \"Example 2: 'Apologies for the inconvenience. We’d love to hear how we can make things right.'\\n\"\n",
    "            # \"Example 3: 'Thank you for your honesty. Please let us know what went wrong.'\\n\"\n",
    "            # \"Example 4: 'We regret the issue you faced. Let us know how we can do better.'\\n\"\n",
    "            # \"Example 5: 'Sincere apologies for the trouble. Your feedback will help us improve.'\\n\"\n",
    "            \"Output only response, no extra text.\\n\\n\"\n",
    "            \"Response:[llm_response]\"\n",
    "        )\n",
    "\n",
    "    return base_prompt + task_instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9joPGvD-XgOg"
   },
   "source": [
    "**Генерация ответа**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UG5dvclM2Rx"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_reply(review, sentiment):\n",
    "    \"\"\"\n",
    "      Генерирует ответ на отзыв при помощи LLM и извлекает из результата лишь сам текст ответа.\n",
    "\n",
    "      Args:\n",
    "        review (str): Текст пользовательского отзыва.\n",
    "        sentiment ({'positive', 'neutral', 'negative'}): Определённая ранее тональность отзыва.\n",
    "\n",
    "      Returns:\n",
    "        str: Сгенерированный ответ (без служебного промпта).\n",
    "          Если по шаблону «Response:[llm_response] ...» ничего не найдено,\n",
    "          функция вернёт исходную генерацию, удалив лишние пробелы.\n",
    "      \"\"\"\n",
    "\n",
    "    response = generator(build_prompt(review, sentiment), max_length=100, temperature=0.9, top_p=0.95, do_sample=True)[0]['generated_text']\n",
    "\n",
    "    match = re.search(r\"Response:\\[llm_response\\](.*)\", response, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        # Чистим от кавычек и пробелов\n",
    "        return match.group(1).strip(' \"\\n')\n",
    "    else:\n",
    "          # Фолбэк: возвращаем как есть\n",
    "        return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mfgCnrCtCUT"
   },
   "source": [
    "**Функция полного процесса анализа и генерации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POZzqJd2PleH"
   },
   "outputs": [],
   "source": [
    "def process_review(review, pipe):\n",
    "    \"\"\"\n",
    "    Полный пайплайн: определяет тональность и генерирует ответ.\n",
    "\n",
    "    Args:\n",
    "        review: Текст отзыва.\n",
    "        pipe: Hugging Face pipeline модели сентимент‑анализа.\n",
    "\n",
    "    Returns:\n",
    "        str: Строка с готовым ответом.\n",
    "    \"\"\"\n",
    "\n",
    "    sentiment = predict_sentiment(review, pipe)\n",
    "    reply = generate_reply(review, sentiment['label'])\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khgMqtfIXmQv"
   },
   "source": [
    "**Обработка одного отзыва**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujMD_TJFQftz"
   },
   "outputs": [],
   "source": [
    "reply = process_review(user_text, pipe)\n",
    "print(\"Ответ:\", reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itm7YNpxsggI"
   },
   "source": [
    "Посчитали некоторые метрики — **ROUGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xtt2dwvsw2HZ"
   },
   "outputs": [],
   "source": [
    "reference_responses = {\n",
    "    \"positive\": [\n",
    "        \"Thank you so much for your kind words! We're thrilled you enjoyed your experience.\",\n",
    "        \"We really appreciate your feedback. Come back again soon!\",\n",
    "        \"Thanks for sharing your positive review! Hope to see you again.\",\n",
    "        \"So glad to hear you’re happy with the product. Thank you!\",\n",
    "        \"Thanks a lot! Your satisfaction means a lot to us.\"\n",
    "    ],\n",
    "    \"neutral\": [\n",
    "        \"Thank you for your feedback. We'll take it into consideration.\",\n",
    "        \"We appreciate your input and will use it to improve our service.\",\n",
    "        \"Thanks for your comment. It helps us make things better.\",\n",
    "        \"We hear you and value your opinion. Thank you.\",\n",
    "        \"Your feedback has been noted. Thank you for taking the time.\"\n",
    "    ],\n",
    "    \"negative\": [\n",
    "        \"We're sorry to hear about your experience. How can we improve?\",\n",
    "        \"Apologies for the inconvenience. We’d love to hear how we can make things right.\",\n",
    "        \"Thank you for your honesty. Please let us know what went wrong.\",\n",
    "        \"We regret the issue you faced. Let us know how we can do better.\",\n",
    "        \"Sincere apologies for the trouble. Your feedback will help us improve.\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJMB-9hvyRt2"
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "rouge_evaluator = load(\"rouge\")\n",
    "\n",
    "def get_rouge(generated, sentiment, reference_dict):\n",
    "    \"\"\"\n",
    "    Посчитать ROUGE‑1 и ROUGE‑L для одного сгенерированного ответа.\n",
    "\n",
    "    Args:\n",
    "        generated: Сгенерированный ответ.\n",
    "        sentiment: Метка тональности (ключ в ``reference_dict``).\n",
    "        reference_dict: Словарь вида ``{'positive': [...], ...}``.\n",
    "\n",
    "    Returns:\n",
    "        dict: {'rouge1': f1, 'rougeL': f1}.\n",
    "    \"\"\"\n",
    "    references = reference_dict[sentiment]\n",
    "\n",
    "    scores = rouge_evaluator.compute(\n",
    "        predictions=[generated],\n",
    "        references=[references],\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "\n",
    "    return {\"rouge1\": scores[\"rouge1\"], \"rougeL\": scores[\"rougeL\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPzKpvOnymnH"
   },
   "outputs": [],
   "source": [
    "rouge_scores = get_rouge(reply, prediction['label'], reference_responses)\n",
    "print(rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ackymc0TTodt"
   },
   "source": [
    "Посчитали некоторые метрики — **Semantic Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "bXxQUexvyp3U"
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIhK5xLWytVp"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')  # лёгкая и быстрая"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKloNMmoyw9g"
   },
   "outputs": [],
   "source": [
    "def get_max_semantic_similarity(generated_text, references):\n",
    "    \"\"\"\n",
    "    Вычислить максимальную семантическую близость между\n",
    "    сгенерированным ответом и списком эталонов.\n",
    "\n",
    "    Args:\n",
    "        generated_text: Ответ, полученный от LLM.\n",
    "        references: Список строк‑эталонов.\n",
    "\n",
    "    Returns:\n",
    "        Наибольшее cosine‑similarity в диапазоне ``[0, 1]``.\n",
    "    \"\"\"\n",
    "\n",
    "    ref_embeddings = semantic_model.encode(references, convert_to_tensor=True)\n",
    "    gen_embedding = semantic_model.encode(generated_text, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(gen_embedding, ref_embeddings)\n",
    "    max_score = float(cosine_scores.max())\n",
    "    return max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAHMsLqyyxk3"
   },
   "outputs": [],
   "source": [
    "sim_score = get_max_semantic_similarity(reply, prediction['label'])\n",
    "print(\"Semantic similarity:\", sim_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZLo68JST993"
   },
   "source": [
    "Тут решил сделать функцию, которая вычислит обе метрики и запишет таблицей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJyx4Ezay0lf"
   },
   "outputs": [],
   "source": [
    "def evaluate_response(reply, sentiment):\n",
    "    \"\"\"\n",
    "    Собрать метрики ROUGE и semantic similarity для ответа.\n",
    "\n",
    "    Args:\n",
    "        reply: Сгенерированный ответ.\n",
    "        sentiment: Тональность (positive / neutral / negative).\n",
    "\n",
    "    Returns:\n",
    "        Словарь с метками и тремя метриками.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    rouge = get_rouge(reply, prediction['label'], reference_responses)\n",
    "    sim = get_max_semantic_similarity(reply, prediction['label'])\n",
    "\n",
    "    results.append({\n",
    "        \"Sentiment\": sentiment,\n",
    "        \"Reply\": reply,\n",
    "        \"ROUGE-1\": rouge[\"rouge1\"],\n",
    "        \"ROUGE-L\": rouge[\"rougeL\"],\n",
    "        \"SemanticSim\": sim\n",
    "    })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdidLTIN0BAo"
   },
   "outputs": [],
   "source": [
    "res_tab = evaluate_response(reply, prediction['label'])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(res_tab)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdAPb4_nUJlF"
   },
   "source": [
    "Как итог метрики поганые😞\n",
    "\n",
    "Как улучшить пока не понимаю"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2Eotk9jUbmj"
   },
   "source": [
    "## Итоги:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLi2gOhHUff1"
   },
   "source": [
    "**Как итог:**\n",
    "1. Объединили и решили две задачи: сентимет анализ и генерацию текста\n",
    "2. Воспользовались различными моделями для различных задач\n",
    "3. Воспользовались различными API\n",
    "4. Ответ на отзыв генерируется, однако оценка ответа низкая\n",
    "5. Работа с промптом особых улучшений не приносит, возможно проблема в модели\n",
    "6. Были созданы пользовательские функции\n",
    "7. В целом можно вырастить из этого какую-то AutoML задачу, чтобы по оценкам ответов LLM, менять промпт или ещё какие параметры, пока метрики не станут качественными, но кажется это займёт много времени."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyODC+Cvk1xLy6WFV4qHJx+i",
   "collapsed_sections": [
    "OFo7QR8GQmmz",
    "F0gss5WacWxm",
    "dW8oOckSWfas",
    "RTZTjoL3Ws1x",
    "POv7mZL_W3gQ",
    "svtQFjR0W_8T",
    "JFaH4sawXLPX",
    "Q-p-IJ5xXXqq",
    "S2Eotk9jUbmj"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
