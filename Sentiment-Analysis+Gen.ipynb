{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFo7QR8GQmmz"
   },
   "source": [
    "## –ò–¥–µ—è:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUGkkHA_QqKP"
   },
   "source": [
    "**–ò–¥–µ—è:** —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–∞ –∫ –æ—Ç–∑—ã–≤—É. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º —Å–æ–≤–º–µ—Å—Ç–∏–≤ –∑–∞–¥–∞—á—É —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç –∞–Ω–∞–ª–∏–∑–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: **RoBERTa** –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ **Mistral** –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n",
    "\n",
    "[–î–∞—Ç–∞—Å–µ—Ç](https://huggingface.co/datasets/Yelp/yelp_review_full):\n",
    "* —Å–æ–¥–µ—Ä–∂–∏—Ç 650–∫/50–∫ –æ—Ç–∑—ã–≤–æ–≤ —Å –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã Yelp, –æ–± —É—Å–ª—É–≥–∞—Ö, —Å–µ—Ä–≤–∏—Å–∞—Ö, —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞—Ö –∏ –ø—Ä.\n",
    "* –∏–º–µ–µ—Ç 5 –∫–ª–∞—Å—Å–æ–≤ ‚Äî 5 –∑–≤—ë–∑–¥\n",
    "* –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0gss5WacWxm"
   },
   "source": [
    "## –°–µ–Ω—Ç–∏–º–µ–Ω—Ç –∞–Ω–∞–ª–∏–∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dW8oOckSWfas"
   },
   "source": [
    "### –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13214,
     "status": "ok",
     "timestamp": 1752185598260,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "HnDOcYfHGd_C",
    "outputId": "4916a802-e872-4574-dd6c-60557823d2a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets huggingface_hub fsspec;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5758,
     "status": "ok",
     "timestamp": 1752185604021,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "2Zlst_g7Dzof",
    "outputId": "94ed88b3-7ecf-4b76-efb2-f74982e14eed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'yelp_review_full' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'yelp_review_full' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"yelp_review_full\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "si33fmiyco33"
   },
   "source": [
    "–ù–µ–º–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç, –¥–ª—è —ç—Ç–æ–≥–æ —Å–æ–∑–¥–∞–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏:\n",
    "1. –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤, —ç–º–æ–¥–∑–∏ –∏ –ø—Ä–æ—á–µ–≥–æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BV1ojNPvEENJ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,!?;:()\\\"'-]\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXZ3-JPmdiqY"
   },
   "source": [
    "2. –ü–µ—Ä–µ—Ä–∞–∑–º–µ—Ç–∏–º –¥–∞–Ω–Ω—ã–µ —Å –æ—Ü–µ–Ω–∫–∏ 0-5, –¥–æ 0-3(–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π, –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUDWW2IhbrHV"
   },
   "outputs": [],
   "source": [
    "def convert_label(example):\n",
    "    rating = example[\"label\"] + 1\n",
    "    if rating in [4, 5]:\n",
    "        label = 2  # positive\n",
    "    elif rating == 3:\n",
    "        label = 1  # neutral\n",
    "    else:\n",
    "        label = 0  # negative\n",
    "    example[\"label\"] = label\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cl132MXcdkHu"
   },
   "source": [
    "–¢—É—Ç –ø–æ—è–≤–∏–ª–∞—Å—å –∏–¥–µ—è –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã, —Ç.–∫. –ª–∞—Ç–∏–Ω–∏—Ü–∞ != –∞–Ω–≥–ª–∏–π—Å–∫–∏–π. –ù–æ –ø–æ—Å–º–æ—Ç—Ä–µ–≤ —Å–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç —ç—Ç–æ –≤—Å—ë –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è, —Å—Ç–∞–ª–æ –Ω–µ —Ç–∞–∫ –≤–∞–∂–Ω–æ, —Ö–æ—Ç—è –¥—É–º–∞—é —ç—Ç–æ –ø–æ–≤—ã—Å–∏–ª–æ –±—ã –º–µ—Ç—Ä–∏–∫–∏.\n",
    "\n",
    "–í–æ–∑–º–æ–∂–Ω–æ —Å—Ç–æ–∏–ª–æ –±—ã —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç —Ä–∞–∑ –≤ 10 —Ö–æ—Ç—è –±—ã, –Ω–æ –∫–æ–≥–¥–∞ —è —ç—Ç–æ –¥–µ–ª–∞–ª, –¥—É–º–∞–ª, –æ —Ç–æ–º, —á—Ç–æ –≤–¥—Ä—É–≥ –≤—Å–µ –ø–æ–ø–∞–≤—à–∏–µ –≤ –≤—ã–±–æ—Ä–∫—É –æ—Ç–∑—ã–≤—ã –±—É–¥—É—Ç –Ω–∞ –ª—é–±–æ–º –∫—Ä–æ–º–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ(–∞–∑–∏–∞—Ç–æ–≤ –∂–µ –º–Ω–æ–≥–æ..) ü§°ü§°\n",
    "\n",
    "–¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –Ω–µ–∫–æ—Ç–æ—Ä—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª, –æ—Å—Ç–∞–ª–æ—Å—å –±—É–∫–≤–∞–ª—å–Ω–æ –≤—ã–±–æ—Ä–∫—É —Å–¥–µ–ª–∞—Ç—å –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ç—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJLKTaOANXAI"
   },
   "outputs": [],
   "source": [
    "# !pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcXYFtjQNGR0"
   },
   "outputs": [],
   "source": [
    "# from langdetect import detect\n",
    "\n",
    "# def is_english(example):\n",
    "#     try:\n",
    "#         return detect(example['text']) == 'en'\n",
    "#     except:\n",
    "#         return False  # –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫, –∏—Å–∫–ª—é—á–∞–µ–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ic0K3bEINFSA"
   },
   "outputs": [],
   "source": [
    "# ds = ds.filter(is_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kt8_TglWeqQP"
   },
   "source": [
    "–ó–¥–µ—Å—å —Å–æ–∫—Ä–∞—Ç–∏–ª–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç, —Ç.–∫. 700–∫ –æ—Ç–∑—ã–≤–æ–≤ —Å–æ–±–∏—Ä–∞–ª–∏—Å—å –æ–±—É—á–∞—Ç—Å—è 5 –¥–Ω–µ–π –Ω–∞ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–º –≤ –∫–æ–ª–ª–∞–±–µ –∂–µ–ª–µ–∑–µ.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ng-2-AL0c38"
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, concatenate_datasets\n",
    "\n",
    "#ds = DatasetDict({\n",
    "#    'train': ds['train'].shuffle(seed=42).select(range(13000)),\n",
    "#    'test': ds['test'].shuffle(seed=42).select(range(1000))\n",
    "#})\n",
    "\n",
    "train_ds = ds['train']\n",
    "\n",
    "pos = train_ds.filter(lambda x: x['label'] in [3, 4])\n",
    "neu = train_ds.filter(lambda x: x['label'] == 2)\n",
    "neg = train_ds.filter(lambda x: x['label'] in [0, 1])\n",
    "\n",
    "pos = pos.shuffle(seed=42).select(range(5000))\n",
    "neu = neu.shuffle(seed=42).select(range(5000))\n",
    "neg = neg.shuffle(seed=42).select(range(5000))\n",
    "test_ds = ds['test'].shuffle(seed=42).select(range(2500))\n",
    "\n",
    "balanced_train = concatenate_datasets([pos, neu, neg]).shuffle(seed=42)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': balanced_train,\n",
    "    'test': test_ds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "2c3e8c0062d940ef85f3208379f2619f",
      "be8891b869054725ac6093261d82fa74",
      "2bae2173d92e4f159d0b2d50c1904a73",
      "7401321685ad46e8b57cc6636dda2672",
      "a8a14ba6be164d038749fc973a8f9084",
      "f0c378808bae4cd0a4fc33093fe1d17a",
      "508d9d79b8cb495d9799d682e97ab151",
      "7449f57aa1e0480b9f10cc11ddcecf3e",
      "16b46b84061e47aca690135e71adce16",
      "8c3318b308024c0d9aa51f81936a7458",
      "39231a21a0a9441088c3efdbba8af8e8",
      "d642840e727d4462a9065ac8a3a37786",
      "faaf8812ec2f4ba281246a50e0b71185",
      "037dd14e3c654d0e818bc9ac23f3f24a",
      "f6f1c4f56fc740f3a14ab09baf476cb9",
      "8ba2ba323e6144d5884511f015436557",
      "5eac28539ad447cd937067a3d7f9747b",
      "8ad344b144ec4873a9287b54f35ce80e",
      "560b736e556c4c49a0b3c33b003acda7",
      "d6b65d97a1cb492fba886b9e0c02a11c",
      "4d62b50963c14a2e8b560f6301b27348",
      "4d485407eb5c4b9296cf0ad6d617a7e4",
      "b50422aa594b46089ef2204ecf7cc875",
      "ce87fa4a31954e9fa101d876ca7f9a36",
      "28092c8d7a714ec2a2e356c35ca90ecb",
      "beaeab966dd44275904dce3df8b2cc64",
      "f6f04c0eef6a4de0806cc373b96b0175",
      "3f35dfeae1dc4e4881f12231c43ef67f",
      "1bdb92d6d06141cabaaee2ae382767ff",
      "01fe552ef3a94a61a0f83d9de583b397",
      "e9694ef2655843d4857030eeb4f87f03",
      "5ecdcac29a3f43db860620b132501748",
      "9b7344bc717b4b7fa34f491556c5f0c6",
      "6b753ce30d324453974b5458883de305",
      "7c4d973e7eca466899234b28ba397bbc",
      "717e4f3be73e4572920858429f779ec2",
      "af5e581ceca04e499da5d12b5af15811",
      "7a0988408ee74ad78e53fd0d4c331036",
      "f9d10529374047dc96b1ab2551e8feed",
      "d367f4b310344781b8f24da92390c7e5",
      "18d8bf2117a04c4ea9c0f0df6723b17d",
      "223caf3b4e544fc5b2b3d229b0ec1c62",
      "bdfbdeb59f7e40118b4f71cb4982c141",
      "08a78870b6f74d68aceca6efeaf1d5b1"
     ]
    },
    "executionInfo": {
     "elapsed": 9991,
     "status": "ok",
     "timestamp": 1752185614455,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "6d5LRPXuLa8c",
    "outputId": "fbce79e7-a5c5-4c63-b908-e63b5247a95c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3e8c0062d940ef85f3208379f2619f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d642840e727d4462a9065ac8a3a37786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50422aa594b46089ef2204ecf7cc875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b753ce30d324453974b5458883de305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.map(lambda x: {\"text\": clean_text(x[\"text\"])})\n",
    "ds = ds.map(convert_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 893,
     "status": "ok",
     "timestamp": 1752185615350,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "ny4HzahP9-F1",
    "outputId": "e502c241-7b87-4e5f-d77c-93703d6bf4ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative: 5000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
      "neutral: 5000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
      "positive: 5000 –ø—Ä–∏–º–µ—Ä–æ–≤\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter(ds['train']['label'])\n",
    "label_names = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "for label_id, count in counter.items():\n",
    "    print(f\"{label_names[label_id]}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTZTjoL3Ws1x"
   },
   "source": [
    "### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ DataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2GnP3SlgkVC"
   },
   "source": [
    "–ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –±—Ä–∞–ª–∏ bert-base, –Ω–æ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ LLM –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å roberta, —è –∏ –ø–æ—Å–ª—É—à–∞–ª—Å—è)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gY8lpMV6QRSg"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], max_length=128, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4QfHXtuinwl"
   },
   "source": [
    "–°–æ–±—Å—Ç–≤–µ–Ω–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "edab00ba0c47495a97b77ba0f020d2f2",
      "24929f7de270407a91603cf5ee0194ab",
      "5dd26e3b8c1a44e59dd23e92ec94d2ff",
      "f3a3b2ee38004521ae58e61e1a90f2ed",
      "dba3dd165a694b45b18858e02fee9f50",
      "b9ed70befad041a9adc11b20c6e8ac70",
      "f9492666fd134bb0ba07fa217070d835",
      "dbda5991288e4fbe922eb7596d90a678",
      "11b7436b33cd46639784eee59aab1dba",
      "17b10e86ac754c1fad5afb1aee31c64a",
      "7a09cec3773b4fdbb17bdbbec5651de6",
      "7fb081ce56df414fb76a76582b24b2ad",
      "9aa16d3d65384266a091445a1c4acf7c",
      "47f9dc063c024d7a8d52db1addef698e",
      "52b48502cbcc4843a7b3ca775acaad13",
      "68127836089e4507aae4089a3e97f821",
      "8cd807094a354c7bbefd05bff7f70617",
      "545119b614a746f6bd9df9d127e96b9d",
      "ab668d38647b4fb180f50900bd329346",
      "ec92220481f04f5fa8f61fe3069ea579",
      "3ae7cb2cca4844e68cf133329f1f4e4b",
      "bf44106616c74625bd985b86440bc94e"
     ]
    },
    "executionInfo": {
     "elapsed": 20058,
     "status": "ok",
     "timestamp": 1752185636345,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "OX6bY1C6j52Z",
    "outputId": "4986a216-e182-4d5c-a3ce-2eae4690a37e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edab00ba0c47495a97b77ba0f020d2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb081ce56df414fb76a76582b24b2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = ds.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kODznMS3kiJH"
   },
   "source": [
    "–î–æ–±–∞–≤–∏–º –ø–∞–¥–¥–∏–Ω–≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFxzOswNoVIl"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POv7mZL_W3gQ"
   },
   "source": [
    "### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 10067,
     "status": "ok",
     "timestamp": 1752185646453,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "FL8IQXSmoFNo",
    "outputId": "0efd70df-c9e5-4bc6-ced2-48a2142681f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GttFP_gblbtE"
   },
   "source": [
    "–£–∫–∞–∑–∞–ª–∏ –º–µ—Ç—Ä–∏–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVLlOiiQj8TS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcu_T6Dqlg4i"
   },
   "source": [
    "–î–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞ üå†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rG0o2i_roDCp"
   },
   "outputs": [],
   "source": [
    "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1752185647168,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "n9EYs9AMrqid",
    "outputId": "90c204e4-a3c1-4c70-c6ff-29b41175d884"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZobaPjm-mApR"
   },
   "source": [
    "Model hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1752185647574,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "HD9WbkhHoyIW",
    "outputId": "4f19655b-ce66-4d13-eb8e-3c1388a7ae19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqvuUMOJr5Lm"
   },
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU2VRaOdmHBB"
   },
   "source": [
    "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBWVUo8UpO94"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuGhWGxImTsc"
   },
   "source": [
    "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1752185650358,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "wYJgYk1CpkuV",
    "outputId": "4d123d9d-26c0-4846-c084-4091669793ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-226-3091136456.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svtQFjR0W_8T"
   },
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weNb6HxRmjSN"
   },
   "source": [
    "–¢—É—Ç —Ä–µ–≥–∞—Ç—å—Å—è –ø—Ä–∏—à–ª–æ—Å—å –∏ –∞–ø–∏—à–∫–∏ –¥–µ–ª–∞—Ç—å(("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "executionInfo": {
     "elapsed": 4106,
     "status": "error",
     "timestamp": 1752185654468,
     "user": {
      "displayName": "Belka Biker",
      "userId": "07317517430318928017"
     },
     "user_tz": -180
    },
    "id": "YQpSwMqisuq8",
    "outputId": "667f47b2-1c16-4537-fc25-71ff40335a13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/1175 : < :, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 143255 has 14.73 GiB memory in use. Of the allocated memory 14.36 GiB is allocated by PyTorch, and 245.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-227-4032920361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3744\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3745\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3747\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3808\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3810\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3811\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1203\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    870\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 )\n\u001b[1;32m    617\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    619\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 143255 has 14.73 GiB memory in use. Of the allocated memory 14.36 GiB is allocated by PyTorch, and 245.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFaH4sawXLPX"
   },
   "source": [
    "### –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlogOo1zms5B"
   },
   "source": [
    "–°–æ–∑–¥–∞–ª–∏ –ø–∞–π–ø –∏ –∑–∞–≥—Ä—É–∑–∏–ª–∏ –ª—É—á—à—É—é –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjM7MC-LtyC7"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=trainer.model,           # –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ Trainer\n",
    "    tokenizer=tokenizer,           # –ø–µ—Ä–µ–¥–∞—ë–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å –º–æ–¥–µ–ª—å—é\n",
    "    device=0                      # —è–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU —Å –∏–Ω–¥–µ–∫—Å–æ–º 0 (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5O9SI9Vm30E"
   },
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2PbwZHJARec"
   },
   "outputs": [],
   "source": [
    "targets = ds[\"test\"][\"label\"]\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsPWuM05AWzS"
   },
   "outputs": [],
   "source": [
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "for out in pipe(KeyDataset(ds[\"test\"], \"text\"), batch_size=16, truncation=True):\n",
    "    predictions.append(label2id[out[\"label\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvkHZWqanu7s"
   },
   "source": [
    "–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "os-v-rSF1D3b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(targets, predictions))\n",
    "\n",
    "cm = confusion_matrix(targets, predictions)\n",
    "ConfusionMatrixDisplay(cm, display_labels=np.array(list(label2id.keys()))).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90YKLbtNn2c2"
   },
   "source": [
    "–¢—É—Ç –ø–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º –æ—Ç–∑—ã–≤–æ–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSXWtstm1KEQ"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(text, pipeline):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "    Args:\n",
    "        text (str): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.\n",
    "        pipeline (transformers.Pipeline): –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "    Returns:\n",
    "        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç —Å –∫–ª—é—á–∞–º–∏ 'label' (–Ω–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞) –∏ 'score' (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å).\n",
    "    \"\"\"\n",
    "    label_map = {\n",
    "    \"LABEL_0\": \"negative\",\n",
    "    \"LABEL_1\": \"neutral\",\n",
    "    \"LABEL_2\": \"positive\"\n",
    "    }\n",
    "\n",
    "    result = pipeline(text)[0]  # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–æ–¥–∏–Ω —Ç–µ–∫—Å—Ç)\n",
    "    label = label_map.get(result['label'], result['label'])\n",
    "    score = result['score']\n",
    "    return {\"label\": label, \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwb4y3Hm2Hx4"
   },
   "outputs": [],
   "source": [
    "user_text = \"I love this product! It works great. You are best!\"\n",
    "\n",
    "prediction = predict_sentiment(user_text, pipe)\n",
    "print(f\"Sentiment: {prediction['label']}, Confidence: {prediction['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qmd6OYQGoBh5"
   },
   "source": [
    "–ù–∞ –æ–¥–Ω–æ–π –∏–∑ –ø–æ–ø—ã—Ç–æ–∫ –±—ã–ª 1.00.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-p-IJ5xXXqq"
   },
   "source": [
    "## –ì–µ–Ω–µ—Ä–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WG1mEu8UjmH"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KqYv9VISoC7"
   },
   "outputs": [],
   "source": [
    "#generator = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.1\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "qW-gVDJvXb1Y"
   },
   "outputs": [],
   "source": [
    "!pip install optimum optimum[onnxruntime] auto-gptq;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXDHYJO4rU2Q"
   },
   "source": [
    "–ó–∞–¥–∞—ë–º **–º–æ–¥–µ–ª—å**\n",
    "\n",
    "–¢—É—Ç –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Mistral, –ø—Ä–∏—à–ª–æ—Å—å –ø—Ä–æ–≤–æ–¥–∏—Ç—å –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∑–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å, —Å–æ–∑–¥–∞–≤–∞—Ç—å —Ç–æ–∫–µ–Ω –∏ —Ç–¥.\n",
    "\n",
    "–ö–∞–∫ –≤–∞—Ä–∏–∞–Ω—Ç –µ—â—ë –±—ã–ª gpt, –Ω–æ –µ–≥–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ; –ì—É–≥–ª–æ–≤—Å–∫–∏–π FLAN-T5, –Ω–æ —Ç—è–∂–µ–ª–æ, –∫–∞–∫ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Mistral, –ø–æ—ç—Ç–æ–º—É –≤–∑—è–ª–∏ GPTQ, –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é, —á—Ç–æ–±—ã –æ–Ω–∞ –≤–ª–µ–∑–ª–∞ –≤ —Ä–µ—Å—É—Ä—Å—ã –∫–æ–ª–∞–±–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zv5H7MZeXRBH"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\", use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSMN5o9EsYOq"
   },
   "source": [
    "–°–æ–∑–¥–∞–ª–∏, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –±–∞–∑–æ–≤—ã–µ **–ø—Ä–æ–º–ø—Ç—ã**\n",
    "\n",
    "–ü—Ä–∏–º–µ—Ä—ã –Ω–∏–∫–∞–∫ –Ω–µ –ø–æ–≤—ã—à–∞—é—Ç –º–µ—Ç—Ä–∏–∫–∏ :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEEZZQwYPedP"
   },
   "outputs": [],
   "source": [
    "def build_prompt(review_text, sentiment_label):\n",
    "    base_prompt = (\n",
    "        f'Review: \"{review_text}\"\\n'\n",
    "        f'You are a customer service assistant. '\n",
    "        f'Generate a short, polite and professional response to the review.\\n'\n",
    "    )\n",
    "\n",
    "    if sentiment_label == 'positive':\n",
    "        task_instruction = (\n",
    "            \"Thank the customer warmly for their feedback and encourage them to return or shop again.\\n\\n\"\n",
    "            # \"Example 1: 'Thank you so much for your kind words! We're thrilled you enjoyed your experience.'\\n\"\n",
    "            # \"Example 2: 'We really appreciate your feedback. Come back again soon!'\\n\"\n",
    "            # \"Example 3: 'Thanks for sharing your positive review! Hope to see you again.\\n'\"\n",
    "            # \"Example 5: 'So glad to hear you‚Äôre happy with the product. Thank you!'\\n\"\n",
    "            # \"Example 6: 'Thanks a lot! Your satisfaction means a lot to us.'\\n\\n\"\n",
    "            # \"Output only response, no extra text.\\n\\n\"\n",
    "            \"Response:[llm_response]\"\n",
    "        )\n",
    "    elif sentiment_label == 'neutral':\n",
    "        task_instruction = (\n",
    "            \"Thank the customer politely and let them know that their feedback will be considered to improve the service.\\n\\n\"\n",
    "            # \"Example 1: 'Thank you for your feedback. We'll take it into consideration.'\\n\"\n",
    "            # \"Example 2: 'We appreciate your input and will use it to improve our service.'\\n\"\n",
    "            # \"Example 3: 'Thanks for your comment. It helps us make things better.'\\n\"\n",
    "            # \"Example 4: 'We hear you and value your opinion. Thank you.'\\n\"\n",
    "            # \"Example 5: 'Your feedback has been noted. Thank you for taking the time.'\\n\\n\"\n",
    "            \"Output only response, no extra text.\\n\\n\"\n",
    "            \"Response:[llm_response]\"\n",
    "        )\n",
    "    else:  # negative\n",
    "        task_instruction = (\n",
    "            \"Apologize sincerely for the negative experience and ask how the service or product can be improved.\\n\\n\"\n",
    "            # \"Example 1: 'We're sorry to hear about your experience. How can we improve?'\\n\"\n",
    "            # \"Example 2: 'Apologies for the inconvenience. We‚Äôd love to hear how we can make things right.'\\n\"\n",
    "            # \"Example 3: 'Thank you for your honesty. Please let us know what went wrong.'\\n\"\n",
    "            # \"Example 4: 'We regret the issue you faced. Let us know how we can do better.'\\n\"\n",
    "            # \"Example 5: 'Sincere apologies for the trouble. Your feedback will help us improve.'\\n\"\n",
    "            \"Output only response, no extra text.\\n\\n\"\n",
    "            \"Response:[llm_response]\"\n",
    "        )\n",
    "\n",
    "    return base_prompt + task_instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9joPGvD-XgOg"
   },
   "source": [
    "**–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UG5dvclM2Rx"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_reply(review, sentiment):\n",
    "    \"\"\"\n",
    "      –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Ç–∑—ã–≤ –ø—Ä–∏ –ø–æ–º–æ—â–∏ LLM –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ª–∏—à—å —Å–∞–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞.\n",
    "\n",
    "      Args:\n",
    "        review (str): –¢–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ—Ç–∑—ã–≤–∞.\n",
    "        sentiment ({'positive', 'neutral', 'negative'}): –û–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è —Ä–∞–Ω–µ–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –æ—Ç–∑—ã–≤–∞.\n",
    "\n",
    "      Returns:\n",
    "        str: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç (–±–µ–∑ —Å–ª—É–∂–µ–±–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞).\n",
    "          –ï—Å–ª–∏ –ø–æ —à–∞–±–ª–æ–Ω—É ¬´Response:[llm_response] ...¬ª –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ,\n",
    "          —Ñ—É–Ω–∫—Ü–∏—è –≤–µ—Ä–Ω—ë—Ç –∏—Å—Ö–æ–¥–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é, —É–¥–∞–ª–∏–≤ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã.\n",
    "      \"\"\"\n",
    "\n",
    "    response = generator(build_prompt(review, sentiment), max_length=100, temperature=0.9, top_p=0.95, do_sample=True)[0]['generated_text']\n",
    "\n",
    "    match = re.search(r\"Response:\\[llm_response\\](.*)\", response, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        # –ß–∏—Å—Ç–∏–º –æ—Ç –∫–∞–≤—ã—á–µ–∫ –∏ –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "        return match.group(1).strip(' \"\\n')\n",
    "    else:\n",
    "          # –§–æ–ª–±—ç–∫: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å\n",
    "        return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mfgCnrCtCUT"
   },
   "source": [
    "**–§—É–Ω–∫—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POZzqJd2PleH"
   },
   "outputs": [],
   "source": [
    "def process_review(review, pipe):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω: –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç.\n",
    "\n",
    "    Args:\n",
    "        review: –¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞.\n",
    "        pipe: Hugging‚ÄØFace pipeline –º–æ–¥–µ–ª–∏ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç‚Äë–∞–Ω–∞–ª–∏–∑–∞.\n",
    "\n",
    "    Returns:\n",
    "        str: –°—Ç—Ä–æ–∫–∞ —Å –≥–æ—Ç–æ–≤—ã–º –æ—Ç–≤–µ—Ç–æ–º.\n",
    "    \"\"\"\n",
    "\n",
    "    sentiment = predict_sentiment(review, pipe)\n",
    "    reply = generate_reply(review, sentiment['label'])\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khgMqtfIXmQv"
   },
   "source": [
    "**–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujMD_TJFQftz"
   },
   "outputs": [],
   "source": [
    "reply = process_review(user_text, pipe)\n",
    "print(\"–û—Ç–≤–µ—Ç:\", reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itm7YNpxsggI"
   },
   "source": [
    "–ü–æ—Å—á–∏—Ç–∞–ª–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ‚Äî **ROUGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xtt2dwvsw2HZ"
   },
   "outputs": [],
   "source": [
    "reference_responses = {\n",
    "    \"positive\": [\n",
    "        \"Thank you so much for your kind words! We're thrilled you enjoyed your experience.\",\n",
    "        \"We really appreciate your feedback. Come back again soon!\",\n",
    "        \"Thanks for sharing your positive review! Hope to see you again.\",\n",
    "        \"So glad to hear you‚Äôre happy with the product. Thank you!\",\n",
    "        \"Thanks a lot! Your satisfaction means a lot to us.\"\n",
    "    ],\n",
    "    \"neutral\": [\n",
    "        \"Thank you for your feedback. We'll take it into consideration.\",\n",
    "        \"We appreciate your input and will use it to improve our service.\",\n",
    "        \"Thanks for your comment. It helps us make things better.\",\n",
    "        \"We hear you and value your opinion. Thank you.\",\n",
    "        \"Your feedback has been noted. Thank you for taking the time.\"\n",
    "    ],\n",
    "    \"negative\": [\n",
    "        \"We're sorry to hear about your experience. How can we improve?\",\n",
    "        \"Apologies for the inconvenience. We‚Äôd love to hear how we can make things right.\",\n",
    "        \"Thank you for your honesty. Please let us know what went wrong.\",\n",
    "        \"We regret the issue you faced. Let us know how we can do better.\",\n",
    "        \"Sincere apologies for the trouble. Your feedback will help us improve.\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJMB-9hvyRt2"
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "rouge_evaluator = load(\"rouge\")\n",
    "\n",
    "def get_rouge(generated, sentiment, reference_dict):\n",
    "    \"\"\"\n",
    "    –ü–æ—Å—á–∏—Ç–∞—Ç—å ROUGE‚Äë1 –∏ ROUGE‚ÄëL –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞.\n",
    "\n",
    "    Args:\n",
    "        generated: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç.\n",
    "        sentiment: –ú–µ—Ç–∫–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (–∫–ª—é—á –≤ ``reference_dict``).\n",
    "        reference_dict: –°–ª–æ–≤–∞—Ä—å –≤–∏–¥–∞ ``{'positive': [...], ...}``.\n",
    "\n",
    "    Returns:\n",
    "        dict: {'rouge1': f1, 'rougeL': f1}.\n",
    "    \"\"\"\n",
    "    references = reference_dict[sentiment]\n",
    "\n",
    "    scores = rouge_evaluator.compute(\n",
    "        predictions=[generated],\n",
    "        references=[references],\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "\n",
    "    return {\"rouge1\": scores[\"rouge1\"], \"rougeL\": scores[\"rougeL\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPzKpvOnymnH"
   },
   "outputs": [],
   "source": [
    "rouge_scores = get_rouge(reply, prediction['label'], reference_responses)\n",
    "print(rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ackymc0TTodt"
   },
   "source": [
    "–ü–æ—Å—á–∏—Ç–∞–ª–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ‚Äî **Semantic Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "bXxQUexvyp3U"
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIhK5xLWytVp"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')  # –ª—ë–≥–∫–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKloNMmoyw9g"
   },
   "outputs": [],
   "source": [
    "def get_max_semantic_similarity(generated_text, references):\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª–∏—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É\n",
    "    —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º –∏ —Å–ø–∏—Å–∫–æ–º —ç—Ç–∞–ª–æ–Ω–æ–≤.\n",
    "\n",
    "    Args:\n",
    "        generated_text: –û—Ç–≤–µ—Ç, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –æ—Ç LLM.\n",
    "        references: –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫‚Äë—ç—Ç–∞–ª–æ–Ω–æ–≤.\n",
    "\n",
    "    Returns:\n",
    "        –ù–∞–∏–±–æ–ª—å—à–µ–µ cosine‚Äësimilarity –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ ``[0, 1]``.\n",
    "    \"\"\"\n",
    "\n",
    "    ref_embeddings = semantic_model.encode(references, convert_to_tensor=True)\n",
    "    gen_embedding = semantic_model.encode(generated_text, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(gen_embedding, ref_embeddings)\n",
    "    max_score = float(cosine_scores.max())\n",
    "    return max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAHMsLqyyxk3"
   },
   "outputs": [],
   "source": [
    "sim_score = get_max_semantic_similarity(reply, prediction['label'])\n",
    "print(\"Semantic similarity:\", sim_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZLo68JST993"
   },
   "source": [
    "–¢—É—Ç —Ä–µ—à–∏–ª —Å–¥–µ–ª–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –≤—ã—á–∏—Å–ª–∏—Ç –æ–±–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –∑–∞–ø–∏—à–µ—Ç —Ç–∞–±–ª–∏—Ü–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJyx4Ezay0lf"
   },
   "outputs": [],
   "source": [
    "def evaluate_response(reply, sentiment):\n",
    "    \"\"\"\n",
    "    –°–æ–±—Ä–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ ROUGE –∏ semantic similarity –¥–ª—è –æ—Ç–≤–µ—Ç–∞.\n",
    "\n",
    "    Args:\n",
    "        reply: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç.\n",
    "        sentiment: –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å (positive / neutral / negative).\n",
    "\n",
    "    Returns:\n",
    "        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∫–∞–º–∏ –∏ —Ç—Ä–µ–º—è –º–µ—Ç—Ä–∏–∫–∞–º–∏.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    rouge = get_rouge(reply, prediction['label'], reference_responses)\n",
    "    sim = get_max_semantic_similarity(reply, prediction['label'])\n",
    "\n",
    "    results.append({\n",
    "        \"Sentiment\": sentiment,\n",
    "        \"Reply\": reply,\n",
    "        \"ROUGE-1\": rouge[\"rouge1\"],\n",
    "        \"ROUGE-L\": rouge[\"rougeL\"],\n",
    "        \"SemanticSim\": sim\n",
    "    })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdidLTIN0BAo"
   },
   "outputs": [],
   "source": [
    "res_tab = evaluate_response(reply, prediction['label'])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(res_tab)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdAPb4_nUJlF"
   },
   "source": [
    "–ö–∞–∫ –∏—Ç–æ–≥ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ–≥–∞–Ω—ã–µüòû\n",
    "\n",
    "–ö–∞–∫ —É–ª—É—á—à–∏—Ç—å –ø–æ–∫–∞ –Ω–µ –ø–æ–Ω–∏–º–∞—é"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2Eotk9jUbmj"
   },
   "source": [
    "## –ò—Ç–æ–≥–∏:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLi2gOhHUff1"
   },
   "source": [
    "**–ö–∞–∫ –∏—Ç–æ–≥:**\n",
    "1. –û–±—ä–µ–¥–∏–Ω–∏–ª–∏ –∏ —Ä–µ—à–∏–ª–∏ –¥–≤–µ –∑–∞–¥–∞—á–∏: —Å–µ–Ω—Ç–∏–º–µ—Ç –∞–Ω–∞–ª–∏–∑ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞\n",
    "2. –í–æ—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á\n",
    "3. –í–æ—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ API\n",
    "4. –û—Ç–≤–µ—Ç –Ω–∞ –æ—Ç–∑—ã–≤ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è, –æ–¥–Ω–∞–∫–æ –æ—Ü–µ–Ω–∫–∞ –æ—Ç–≤–µ—Ç–∞ –Ω–∏–∑–∫–∞—è\n",
    "5. –†–∞–±–æ—Ç–∞ —Å –ø—Ä–æ–º–ø—Ç–æ–º –æ—Å–æ–±—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π –Ω–µ –ø—Ä–∏–Ω–æ—Å–∏—Ç, –≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–±–ª–µ–º–∞ –≤ –º–æ–¥–µ–ª–∏\n",
    "6. –ë—ã–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "7. –í —Ü–µ–ª–æ–º –º–æ–∂–Ω–æ –≤—ã—Ä–∞—Å—Ç–∏—Ç—å –∏–∑ —ç—Ç–æ–≥–æ –∫–∞–∫—É—é-—Ç–æ AutoML –∑–∞–¥–∞—á—É, —á—Ç–æ–±—ã –ø–æ –æ—Ü–µ–Ω–∫–∞–º –æ—Ç–≤–µ—Ç–æ–≤ LLM, –º–µ–Ω—è—Ç—å –ø—Ä–æ–º–ø—Ç –∏–ª–∏ –µ—â—ë –∫–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –ø–æ–∫–∞ –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ —Å—Ç–∞–Ω—É—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏, –Ω–æ –∫–∞–∂–µ—Ç—Å—è —ç—Ç–æ –∑–∞–π–º—ë—Ç –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyODC+Cvk1xLy6WFV4qHJx+i",
   "collapsed_sections": [
    "OFo7QR8GQmmz",
    "F0gss5WacWxm",
    "dW8oOckSWfas",
    "RTZTjoL3Ws1x",
    "POv7mZL_W3gQ",
    "svtQFjR0W_8T",
    "JFaH4sawXLPX",
    "Q-p-IJ5xXXqq",
    "S2Eotk9jUbmj"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
